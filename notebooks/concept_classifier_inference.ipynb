{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# CLIP-Cues: Concept Bottleneck Model - Inference Example\n",
    "\n",
    "This notebook demonstrates how to use pre-trained Concept Bottleneck Models to detect synthetic (AI-generated) images with interpretable predictions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Loading a pre-trained concept bottleneck model\n",
    "2. Running inference on sample images\n",
    "3. Interpreting the results using human-readable concepts\n",
    "4. Understanding which concepts contribute to predictions\n",
    "\n",
    "Unlike the CLIP Orthogonal models, Concept Bottleneck Models provide interpretable predictions by using a vocabulary of human-readable concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required package if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install clip-cues\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConceptClassifierInference' from 'clip_cues' (/workspaces/clip-cues/src/clip_cues/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip_cues\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     ConceptClassifierInference,\n\u001b[32m     10\u001b[39m     CLIPLargePatch14,\n\u001b[32m     11\u001b[39m     ConceptSelectionHead,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip_cues\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transforms\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip_cues\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcepts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConceptVocabulary\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ConceptClassifierInference' from 'clip_cues' (/workspaces/clip-cues/src/clip_cues/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from clip_cues import (\n",
    "    ConceptClassifierInference,\n",
    "    CLIPLargePatch14,\n",
    "    ConceptSelectionHead,\n",
    ")\n",
    "from clip_cues.transforms import Transforms\n",
    "from clip_cues.concepts import ConceptVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load Feature Extractor and Transforms\n",
    "\n",
    "We use CLIP ViT-L/14 as the feature extractor, frozen to preserve its pre-trained vision-language knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP feature extractor\n",
    "extractor = CLIPLargePatch14(cache_dir=\"../hf_cache\")\n",
    "extractor.freeze()\n",
    "\n",
    "# Setup image transforms\n",
    "transforms = Transforms(extractor.transforms)\n",
    "inference_transforms = transforms.get_inference_transforms()\n",
    "\n",
    "print(f\"Feature extractor output dimension: {extractor.output_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Concept Vocabulary\n",
    "\n",
    "The concept vocabulary defines the human-readable concepts used for classification. We use antonym pairs (e.g., \"natural\" vs \"artificial\", \"organic\" vs \"synthetic\") to create a rich concept space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concept vocabulary\n",
    "vocab = ConceptVocabulary.load_antonyms()\n",
    "\n",
    "# Create concept embeddings using CLIP\n",
    "concept_embeddings = vocab.create_embeddings(extractor.model)\n",
    "\n",
    "print(f\"Number of concepts: {len(vocab.concepts)}\")\n",
    "print(f\"Concept embedding dimension: {concept_embeddings.shape}\")\n",
    "print(f\"\\nExample concepts:\")\n",
    "for i, concept in enumerate(vocab.concepts[:10]):\n",
    "    print(f\"  {i+1}. {concept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load Classification Head\n",
    "\n",
    "The Concept Selection Head learns which concepts are relevant for detecting synthetic images and uses them to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the concept selection head\n",
    "head = ConceptSelectionHead(\n",
    "    num_concepts=len(vocab.concepts),\n",
    "    concept_embeddings=concept_embeddings,\n",
    ")\n",
    "\n",
    "print(f\"Concept selection head initialized with {len(vocab.concepts)} concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Create Inference Model and Load Weights\n",
    "\n",
    "Available pre-trained concept models:\n",
    "- `cm_antonyms_cnnspot.ckpt` - Trained on CNNSpot dataset\n",
    "- `cm_antonyms_synthbuster.ckpt` - Trained on SynthBuster+ dataset\n",
    "- `cm_antonyms_synthclic.ckpt` - Trained on SynthCLIC dataset\n",
    "- `cm_antonyms_combined.ckpt` - Trained on combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full inference model\n",
    "model = ConceptClassifierInference(extractor.model, head)\n",
    "\n",
    "# Load pre-trained weights\n",
    "checkpoint_path = \"../data/checkpoints/cm_antonyms_combined.ckpt\"\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if Path(checkpoint_path).exists():\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "\n",
    "    # Remove \"model.\" prefix from state dict keys\n",
    "    weights = {k.replace(\"model.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
    "\n",
    "    # Load weights\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    print(\"✓ Pre-trained weights loaded successfully\")\n",
    "else:\n",
    "    print(f\"⚠ Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please download pre-trained checkpoints from the repository\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Inference on a Single Image with Concept Attribution\n",
    "\n",
    "Let's test the model on a sample image and see which concepts contribute to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image (replace with your own image path)\n",
    "image_path = \"../examples/images/synthetic2.jpg\"\n",
    "\n",
    "if Path(image_path).exists():\n",
    "    # Load and display the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare the image for inference\n",
    "    batch = inference_transforms({\"image\": [image]})\n",
    "    pixel_values = torch.stack(batch[\"pixel_values\"]).to(device)\n",
    "\n",
    "    # Run inference and get concept activations\n",
    "    with torch.no_grad():\n",
    "        prob, concept_scores = model(pixel_values, return_concepts=True)\n",
    "\n",
    "    # Display results\n",
    "    synthetic_prob = prob.item()\n",
    "    real_prob = 1 - synthetic_prob\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Prediction Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Synthetic probability: {synthetic_prob:.3f}\")\n",
    "    print(f\"Real probability:      {real_prob:.3f}\")\n",
    "    print(f\"\\nPrediction: {'SYNTHETIC' if synthetic_prob > 0.5 else 'REAL'}\")\n",
    "    print(f\"Confidence: {max(synthetic_prob, real_prob):.1%}\")\n",
    "    print(f\"{'='*50}\")\n",
    "else:\n",
    "    print(f\"⚠ Image not found at {image_path}\")\n",
    "    print(\"Please provide a valid image path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Visualize Top Contributing Concepts\n",
    "\n",
    "Let's see which concepts most strongly influenced the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(image_path).exists():\n",
    "    # Get concept scores and gates\n",
    "    concept_scores_np = concept_scores.cpu().numpy()[0]\n",
    "    gates = model.head.gates.cpu().numpy()\n",
    "\n",
    "    # Calculate weighted concept scores\n",
    "    weighted_scores = concept_scores_np * gates\n",
    "\n",
    "    # Get top contributing concepts\n",
    "    top_k = 15\n",
    "    top_indices = np.argsort(np.abs(weighted_scores))[-top_k:][::-1]\n",
    "\n",
    "    # Plot top concepts\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    concepts_top = [vocab.concepts[i] for i in top_indices]\n",
    "    scores_top = [weighted_scores[i] for i in top_indices]\n",
    "\n",
    "    colors = ['red' if s > 0 else 'green' for s in scores_top]\n",
    "\n",
    "    y_pos = np.arange(len(concepts_top))\n",
    "    ax.barh(y_pos, scores_top, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(concepts_top)\n",
    "    ax.set_xlabel('Weighted Concept Score', fontsize=12)\n",
    "    ax.set_title(f'Top {top_k} Contributing Concepts\\n(Red = Synthetic, Green = Real)', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed concept analysis\n",
    "    print(f\"\\nTop {top_k} Contributing Concepts:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Rank':<6} {'Concept':<30} {'Score':<12} {'Gate':<12} {'Weighted':<12}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        concept = vocab.concepts[idx]\n",
    "        score = concept_scores_np[idx]\n",
    "        gate = gates[idx]\n",
    "        weighted = weighted_scores[idx]\n",
    "        direction = \"→ Synth\" if weighted > 0 else \"→ Real\"\n",
    "        print(f\"{rank:<6} {concept:<30} {score:>8.4f}     {gate:>8.4f}     {weighted:>8.4f}  {direction}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Batch Inference on Multiple Images\n",
    "\n",
    "Process multiple images at once for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process multiple images from a directory\n",
    "image_dir = Path(\"../examples/images/\")\n",
    "\n",
    "if image_dir.exists():\n",
    "    # Load all images\n",
    "    image_paths = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "    if image_paths:\n",
    "        images = [Image.open(p) for p in image_paths]\n",
    "\n",
    "        # Transform all images\n",
    "        batch = inference_transforms({\"image\": images})\n",
    "        pixel_values = torch.stack(batch[\"pixel_values\"]).to(device)\n",
    "\n",
    "        # Run batch inference\n",
    "        with torch.no_grad():\n",
    "            probs, _ = model(pixel_values, return_concepts=True)\n",
    "\n",
    "        # Display results\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
    "        if len(images) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, (img_path, img, prob) in enumerate(zip(image_paths, images, probs)):\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "            synthetic_prob = prob.item()\n",
    "            prediction = \"SYNTHETIC\" if synthetic_prob > 0.5 else \"REAL\"\n",
    "            confidence = max(synthetic_prob, 1 - synthetic_prob)\n",
    "\n",
    "            color = 'red' if prediction == \"SYNTHETIC\" else 'green'\n",
    "            axes[idx].set_title(\n",
    "                f\"{img_path.name}\\n{prediction} ({confidence:.1%})\",\n",
    "                color=color,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print detailed results\n",
    "        print(\"\\nDetailed Results:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for img_path, prob in zip(image_paths, probs):\n",
    "            synthetic_prob = prob.item()\n",
    "            print(f\"{img_path.name:30} | Synthetic: {synthetic_prob:.3f} | {('SYNTHETIC' if synthetic_prob > 0.5 else 'REAL'):9}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(\"No images found in the directory\")\n",
    "else:\n",
    "    print(f\"Directory {image_dir} not found\")\n",
    "    print(\"Create a 'test_images' folder and add some images to test batch inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Helper Function for Easy Inference with Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_with_concepts(image_path_or_pil, threshold=0.5, top_k=10):\n",
    "    \"\"\"\n",
    "    Predict whether an image is synthetic or real with concept attribution.\n",
    "\n",
    "    Args:\n",
    "        image_path_or_pil: Path to image file or PIL Image object\n",
    "        threshold: Classification threshold (default: 0.5)\n",
    "        top_k: Number of top concepts to return (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results with probabilities, label, and top concepts\n",
    "    \"\"\"\n",
    "    # Load image if path is provided\n",
    "    if isinstance(image_path_or_pil, (str, Path)):\n",
    "        image = Image.open(image_path_or_pil)\n",
    "    else:\n",
    "        image = image_path_or_pil\n",
    "\n",
    "    # Transform and prepare for inference\n",
    "    batch = inference_transforms({\"image\": [image]})\n",
    "    pixel_values = torch.stack(batch[\"pixel_values\"]).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        prob, concept_scores = model(pixel_values, return_concepts=True)\n",
    "\n",
    "    synthetic_prob = prob.item()\n",
    "    real_prob = 1 - synthetic_prob\n",
    "\n",
    "    # Get top concepts\n",
    "    concept_scores_np = concept_scores.cpu().numpy()[0]\n",
    "    gates = model.head.gates.cpu().numpy()\n",
    "    weighted_scores = concept_scores_np * gates\n",
    "\n",
    "    top_indices = np.argsort(np.abs(weighted_scores))[-top_k:][::-1]\n",
    "    top_concepts = [\n",
    "        {\n",
    "            \"concept\": vocab.concepts[i],\n",
    "            \"score\": float(concept_scores_np[i]),\n",
    "            \"gate\": float(gates[i]),\n",
    "            \"weighted_score\": float(weighted_scores[i]),\n",
    "            \"direction\": \"synthetic\" if weighted_scores[i] > 0 else \"real\"\n",
    "        }\n",
    "        for i in top_indices\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"synthetic_probability\": synthetic_prob,\n",
    "        \"real_probability\": real_prob,\n",
    "        \"prediction\": \"synthetic\" if synthetic_prob > threshold else \"real\",\n",
    "        \"confidence\": max(synthetic_prob, real_prob),\n",
    "        \"top_concepts\": top_concepts\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# result = predict_image_with_concepts(\"path/to/image.jpg\")\n",
    "# print(f\"Prediction: {result['prediction']} (confidence: {result['confidence']:.1%})\")\n",
    "# print(f\"\\nTop concepts:\")\n",
    "# for c in result['top_concepts'][:5]:\n",
    "#     print(f\"  - {c['concept']}: {c['weighted_score']:.4f} ({c['direction']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "### Concept Scores\n",
    "\n",
    "- **Concept Score**: Raw similarity between the image and each concept (from CLIP)\n",
    "- **Gate**: Learned weight indicating how relevant each concept is for classification\n",
    "- **Weighted Score**: Concept Score × Gate - the final contribution to the prediction\n",
    "  - Positive values → push toward \"synthetic\"\n",
    "  - Negative values → push toward \"real\"\n",
    "\n",
    "### Interpreting Concepts\n",
    "\n",
    "The model learns which concepts are most informative for distinguishing synthetic from real images:\n",
    "- **High positive weights**: Concepts strongly associated with synthetic images\n",
    "- **High negative weights**: Concepts strongly associated with real images\n",
    "- **Low weights**: Concepts not useful for this classification task\n",
    "\n",
    "### Model Selection Tips\n",
    "\n",
    "- **SynthCLIC**: Best for web images and diverse generative models\n",
    "- **SynthBuster+**: Good for social media images\n",
    "- **CNNSpot**: Specialized for specific GAN architectures\n",
    "- **Combined**: Best overall generalization across different sources\n",
    "\n",
    "### Advantages of Concept Models\n",
    "\n",
    "1. **Interpretability**: See which visual concepts drive predictions\n",
    "2. **Debugging**: Understand when and why the model might fail\n",
    "3. **Trust**: Validate that predictions are based on meaningful visual cues\n",
    "4. **Insights**: Learn what visual characteristics distinguish synthetic images\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Concept vocabulary is predefined (though customizable)\n",
    "- Performance may vary on heavily compressed or edited images\n",
    "- Model trained on specific generative models may not generalize to all new methods\n",
    "- Always validate results in critical applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Comparing Predictions Across Models\n",
    "\n",
    "You can load multiple checkpoints to compare how different training datasets affect concept selection and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare predictions from different checkpoints\n",
    "# checkpoint_names = [\n",
    "#     \"cm_antonyms_cnnspot.ckpt\",\n",
    "#     \"cm_antonyms_synthbuster.ckpt\",\n",
    "#     \"cm_antonyms_synthclic.ckpt\",\n",
    "#     \"cm_antonyms_combined.ckpt\"\n",
    "# ]\n",
    "#\n",
    "# for ckpt_name in checkpoint_names:\n",
    "#     checkpoint_path = f\"../data/checkpoints/{ckpt_name}\"\n",
    "#     if Path(checkpoint_path).exists():\n",
    "#         # Load checkpoint\n",
    "#         checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "#         weights = {k.replace(\"model.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
    "#         model.load_state_dict(weights, strict=False)\n",
    "#         model.eval()\n",
    "#\n",
    "#         # Run inference\n",
    "#         result = predict_image_with_concepts(image_path, top_k=5)\n",
    "#\n",
    "#         print(f\"\\n{ckpt_name}:\")\n",
    "#         print(f\"  Prediction: {result['prediction']} ({result['confidence']:.1%})\")\n",
    "#         print(f\"  Top concepts:\")\n",
    "#         for c in result['top_concepts']:\n",
    "#             print(f\"    - {c['concept']}: {c['weighted_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Compare concept model predictions with CLIP Orthogonal models\n",
    "- Try different pre-trained checkpoints to see how concept selection varies\n",
    "- Create custom concept vocabularies for your specific use case\n",
    "- Fine-tune models on your own datasets\n",
    "- Analyze concept patterns across large image collections\n",
    "- Check the [documentation](https://github.com/marco-willi/clip-cues) for more advanced usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
